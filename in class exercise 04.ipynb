{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28cfa32b",
   "metadata": {},
   "source": [
    "The fourth in-class-exercise (40 points in total, 03/29/2022)\n",
    "Question description: Please use the text corpus you collected in your last in-class-exercise for this exercise. Perform the following tasks:\n",
    "(1) (10 points) Generate K topics by using LDA, the number of topics K should be decided by the coherence score, then summarize what are the topics. You may refer the code here:\n",
    "https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d99fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import lxml.html as LH\n",
    "import urllib.request as urllib2\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def getAllDoxyDonkeyPosts(url,links):\n",
    "    request = urllib2.Request(url)\n",
    "    response = urllib2.urlopen(request)\n",
    "    soup = BeautifulSoup(response)\n",
    "    for a in soup.findAll('a'):\n",
    "        try:\n",
    "            url = a['href']\n",
    "            title = a['title']\n",
    "            if title == \"Older Posts\":\n",
    "                print(title, url)\n",
    "                links.append(url)\n",
    "                getAllDoxyDonkeyPosts(url,links)\n",
    "        except:\n",
    "            title = \"\"\n",
    "    return\n",
    "\n",
    "blogUrl = \"https://doxydonkey.blogspot.com/\"\n",
    "links = []\n",
    "getAllDoxyDonkeyPosts(blogUrl,links)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6778a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDoxyDonkeyText(testUrl):\n",
    "    request = urllib2.Request(testUrl)\n",
    "    response = urllib2.urlopen(request)\n",
    "    soup = BeautifulSoup(response)\n",
    "    mydivs = soup.findAll(\"div\", {\"class\":'post-body'})\n",
    "    \n",
    "\n",
    "    posts =[]\n",
    "    for div in mydivs:\n",
    "        posts+=map(lambda p:str(str(p.text).encode('ascii', errors='replace')).replace(\"?\",\" \"), div.findAll(\"li\"))\n",
    "    return posts\n",
    "\n",
    "doxyDonkeyPosts = []\n",
    "for link in links:\n",
    "    doxyDonkeyPosts+=getDoxyDonkeyText(link)\n",
    "\n",
    "\n",
    "import nltk; \n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf97de6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "data = doxyDonkeyPosts\n",
    "data = [x[2:] for x in data]\n",
    "data = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]\n",
    "\n",
    "# Remove new line characters\n",
    "data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
    "\n",
    "# Remove distracting single quotes\n",
    "data = [re.sub(\"\\'\", \"\", sent) for sent in data]\n",
    "\n",
    "print(data[:1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f240512",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(data))\n",
    "\n",
    "print(data_words[:1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3def39eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# See trigram example\n",
    "print(trigram_mod[bigram_mod[data_words[0]]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659702ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out\n",
    "\n",
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# python3 -m spacy download en\n",
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "print(data_lemmatized[:1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2054cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# View\n",
    "print(corpus[:1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3eb3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Human readable format of corpus (term-frequency)\n",
    "[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b02e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LDA model\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=20, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)\n",
    "\n",
    "# Print the Keyword in the 20 topics\n",
    "pprint(ldamodel.print_topics())\n",
    "doclda = ldamodel[corpus]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bda7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', ldamodel.log_perplexity(corpus)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261542c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Coherence Score\n",
    "coherence_modellda = CoherenceModel(model=ldamodel, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherencelda = coherence_modellda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherencelda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe85833",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pyLDAvis\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "# feed the LDA model into the pyLDAvis instance\n",
    "vis = gensimvis.prepare(lda_model, corpus, id2word)\n",
    "vis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58a8b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download File: http://mallet.cs.umass.edu/dist/mallet-2.0.8.zip\n",
    "\n",
    "mallet_path = '/Downloads/mallet-2.0.8/bin/mallet' # update this path\n",
    "ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=20, id2word=id2word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c56ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Show Topics\n",
    "pprint(ldamallet.show_topics(formatted=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154401f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Coherence Score\n",
    "coherencemodel_ldamallet = CoherenceModel(model=ldamallet, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherenceldamallet = coherencemodel_ldamallet.get_coherence()\n",
    "print('\\nCoherence Score: ', coherenceldamallet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54b5efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    limit : Max num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=num_topics, id2word=id2word)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values\n",
    "\n",
    "model_list, coherence_values = compute_coherence_values(dictionary=id2word, corpus=corpus, texts=data_lemmatized, start=2, limit=40, step=6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e15705a",
   "metadata": {},
   "outputs": [],
   "source": [
    "limit=40; start=2; step=6;\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcb2052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the coherence scores\n",
    "for m, cv in zip(x, coherence_values):\n",
    "    print(\"Num Topics =\", m, \" has Coherence Value of\", round(cv, 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd282666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the model and print the topics\n",
    "optimalmodel = model_list[3]\n",
    "modeltopics = optimalmodel.show_topics(formatted=False)\n",
    "pprint(optimalmodel.print_topics(num_words=10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d5d343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the dominant topic in each sentence\n",
    "def format_topics_sentences(ldamodel=ldamodel, corpus=corpus, texts=data):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n",
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=optimal_model, corpus=corpus, texts=data)\n",
    "\n",
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "\n",
    "# Show\n",
    "df_dominant_topic.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46999200",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_topics_sorteddf_mallet = pd.DataFrame()\n",
    "\n",
    "sent_topics_outdf_grpd = df_topic_sents_keywords.groupby('Dominant_Topic')\n",
    "\n",
    "for i, grp in sent_topics_outdf_grpd:\n",
    "    sent_topics_sorteddf_mallet = pd.concat([sent_topics_sorteddf_mallet, \n",
    "                                             grp.sort_values(['Perc_Contribution'], ascending=[0]).head(1)], \n",
    "                                            axis=0)\n",
    "\n",
    "# Reset Index    \n",
    "sent_topics_sorteddf_mallet.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Format\n",
    "sent_topics_sorteddf_mallet.columns = ['Topic_Num', \"Topic_Perc_Contrib\", \"Keywords\", \"Text\"]\n",
    "\n",
    "# Show\n",
    "sent_topics_sorteddf_mallet.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9542b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topic distribution across documents\n",
    "# Number of Documents for Each Topic\n",
    "topiccounts = df_topic_sents_keywords['Dominant_Topic'].value_counts()\n",
    "\n",
    "# Percentage of Documents for Each Topic\n",
    "topiccontribution = round(topic_counts/topic_counts.sum(), 4)\n",
    "\n",
    "# Topic Number and Keywords\n",
    "topicnum_keywords = df_topic_sents_keywords[['Dominant_Topic', 'Topic_Keywords']]\n",
    "\n",
    "# Concatenate Column wise\n",
    "dfdominant_topics = pd.concat([topic_num_keywords, topic_counts, topic_contribution], axis=1)\n",
    "\n",
    "# Change Column names\n",
    "dfdominant_topics.columns = ['Dominant_Topic', 'Topic_Keywords', 'Num_Documents', 'Perc_Documents']\n",
    "\n",
    "# Show\n",
    "dfdominant_topics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d598895e",
   "metadata": {},
   "source": [
    "2) (10 points) Generate K topics by using LSA, the number of topics K should be decided by the coherence score, then summarize what are the topics. You may refer the code here:\n",
    "https://www.datacamp.com/community/tutorials/discovering-hidden-topics-python\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7579fb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import lxml.html as LH\n",
    "import urllib.request as urllib2\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def getAllDoxyDonkeyPosts(url,links):\n",
    "    request = urllib2.Request(url)\n",
    "    response = urllib2.urlopen(request)\n",
    "    soup = BeautifulSoup(response)\n",
    "    for a in soup.findAll('a'):\n",
    "        try:\n",
    "            url = a['href']\n",
    "            title = a['title']\n",
    "            if title == \"Older Posts\":\n",
    "                print(title, url)\n",
    "                links.append(url)\n",
    "                getAllDoxyDonkeyPosts(url,links)\n",
    "        except:\n",
    "            title = \"\"\n",
    "    return\n",
    "\n",
    "blogUrl = \"https://doxydonkey.blogspot.com/\"\n",
    "links = []\n",
    "getAllDoxyDonkeyPosts(blogUrl,links)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1759fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDoxyDonkeyText(testUrl):\n",
    "    request = urllib2.Request(testUrl)\n",
    "    response = urllib2.urlopen(request)\n",
    "    soup = BeautifulSoup(response)\n",
    "    mydivs = soup.findAll(\"div\", {\"class\":'post-body'})\n",
    "    \n",
    "\n",
    "    posts =[]\n",
    "    for div in mydivs:\n",
    "        posts+=map(lambda p:str(str(p.text).encode('ascii', errors='replace')).replace(\"?\",\" \"), div.findAll(\"li\"))\n",
    "    return posts\n",
    "\n",
    "doxyDonkeyPosts = []\n",
    "for link in links:\n",
    "    doxyDonkeyPosts+=getDoxyDonkeyText(link)\n",
    "\n",
    "\n",
    "import nltk; \n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef97a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "data = doxyDonkeyPosts\n",
    "data = [x[2:] for x in data]\n",
    "data = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]\n",
    "\n",
    "# Remove new line characters\n",
    "data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
    "\n",
    "# Remove distracting single quotes\n",
    "data = [re.sub(\"\\'\", \"\", sent) for sent in data]\n",
    "\n",
    "print(data[:1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8273a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(data))\n",
    "\n",
    "print(data_words[:1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f18fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# See trigram example\n",
    "print(trigram_mod[bigram_mod[data_words[0]]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cf44de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out\n",
    "\n",
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# python3 -m spacy download en\n",
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "print(data_lemmatized[:1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805a9cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "LSA code\n",
    "from gensim.models import LsiModel\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "doctermmatrix = [dictionary.doc2bow(doc) for doc in texts]\n",
    "\n",
    "\n",
    "def prepare_corpus(texts):\n",
    "    \"\"\"\n",
    "    Input  : clean document\n",
    "    Purpose: create term dictionary of our courpus and Converting list of documents (corpus) into Document Term Matrix\n",
    "    Output : term dictionary and Document Term Matrix\n",
    "    \"\"\"\n",
    "    # Creating the term dictionary of our courpus, where every unique term is assigned an index. dictionary = corpora.Dictionary(doc_clean)\n",
    "    dictionary = corpora.Dictionary(texts)\n",
    "    # Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
    "    doctermmatrix = [dictionary.doc2bow(doc) for doc in texts]\n",
    "    # generate LDA model\n",
    "    return dictionary,doctermmatrix\n",
    "\n",
    "def create_gensim_lsa_model(texts,number_of_topics,words):\n",
    "    \"\"\"\n",
    "    Input  : clean document, number of topics and number of words associated with each topic\n",
    "    Purpose: create LSA model using gensim\n",
    "    Output : return LSA model\n",
    "    \"\"\"\n",
    "    dictionary,doc_term_matrix=prepare_corpus(texts)\n",
    "    # generate LSA model\n",
    "    lsamodel = LsiModel(doctermmatrix, num_topics=20, id2word = dictionary)  # train model\n",
    "    print(lsamodel.print_topics(num_topics=20, num_words=words))\n",
    "    return lsamodel\n",
    "\n",
    "def compute_coherence_values(dictionary, doc_term_matrix, texts, stop, start=2, step=3):\n",
    "    \"\"\"\n",
    "    Input   : dictionary : Gensim dictionary\n",
    "              corpus : Gensim corpus\n",
    "              texts : List of input texts\n",
    "              stop : Max num of topics\n",
    "    purpose : Compute c_v coherence for various number of topics\n",
    "    Output  : model_list : List of LSA topic models\n",
    "              coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, stop, step):\n",
    "        # generate LSA model\n",
    "        model = LsiModel(doc_term_matrix, num_topics=20, id2word = dictionary)  # train model\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "    return model_list, coherence_values\n",
    "def plot_graph(texts,start, stop, step):\n",
    "    dictionary,doc_term_matrix=prepare_corpus(texts)\n",
    "    model_list, coherence_values = compute_coherence_values(dictionary, doc_term_matrix,texts,\n",
    "                                                            stop, start, step)\n",
    "    # Show graph\n",
    "    x = range(start, stop, step)\n",
    "    plt.plot(x, coherence_values)\n",
    "    plt.xlabel(\"Number of Topics\")\n",
    "    plt.ylabel(\"Coherence score\")\n",
    "    plt.legend((\"coherence_values\"), loc='best')\n",
    "    plt.show()\n",
    "\n",
    "start,stop,step=2,12,1\n",
    "plot_graph(texts,start,stop,step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af0f8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=create_gensim_lsa_model(texts,6,10)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc02de2",
   "metadata": {},
   "source": [
    "(3) (10 points) Generate K topics by using lda2vec, the number of topics K should be decided by the coherence score, then summarize what are the topics. You may refer the code here:\n",
    "https://nbviewer.org/github/cemoody/lda2vec/blob/master/examples/twenty_newsgroups/lda2vec/lda2vec.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d648c17",
   "metadata": {},
   "source": [
    "# Write your code here\n",
    "lda2vec is an extension of word2vec and LDA that jointly learns word, document, and topic vectors.\n",
    "Lda2vec is obtained by modifying the skip-gram word2vec variant. In the original skip-gram method, the model is trained to predict context words based on a pivot word. In lda2vec, the pivot word vector and a document vector are added to obtain a context vector. This context vector is then used to predict context words.\n",
    "I am not getting any proper resources to solve this problem.\n",
    "As from few online resources LDA is better than lda2vec.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86afdded",
   "metadata": {},
   "source": [
    "(4) (10 points) Generate K topics by using BERTopic, the number of topics K should be decided by the coherence score, then summarize what are the topics. You may refer the code here:\n",
    "https://colab.research.google.com/drive/1FieRA9fLdkQEGDIMYl0I3MCjSUKVF8C-?usp=sharing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa01fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import lxml.html as LH\n",
    "import urllib.request as urllib2\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def getAllDoxyDonkeyPosts(url,links):\n",
    "    request = urllib2.Request(url)\n",
    "    response = urllib2.urlopen(request)\n",
    "    soup = BeautifulSoup(response)\n",
    "    for a in soup.findAll('a'):\n",
    "        try:\n",
    "            url = a['href']\n",
    "            title = a['title']\n",
    "            if title == \"Older Posts\":\n",
    "                print(title, url)\n",
    "                links.append(url)\n",
    "                getAllDoxyDonkeyPosts(url,links)\n",
    "        except:\n",
    "            title = \"\"\n",
    "    return\n",
    "\n",
    "blogUrl = \"https://doxydonkey.blogspot.com/\"\n",
    "links = []\n",
    "getAllDoxyDonkeyPosts(blogUrl,links)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e153ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDoxyDonkeyText(testUrl):\n",
    "    request = urllib2.Request(testUrl)\n",
    "    response = urllib2.urlopen(request)\n",
    "    soup = BeautifulSoup(response)\n",
    "    mydivs = soup.findAll(\"div\", {\"class\":'post-body'})\n",
    "    \n",
    "\n",
    "    posts =[]\n",
    "    for div in mydivs:\n",
    "        posts+=map(lambda p:str(str(p.text).encode('ascii', errors='replace')).replace(\"?\",\" \"), div.findAll(\"li\"))\n",
    "    return posts\n",
    "\n",
    "doxyDonkeyPosts = []\n",
    "for link in links:\n",
    "    doxyDonkeyPosts+=getDoxyDonkeyText(link)\n",
    "\n",
    "\n",
    "import nltk; \n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628ef632",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "data = doxyDonkeyPosts\n",
    "data = [x[2:] for x in data]\n",
    "data = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]\n",
    "\n",
    "# Remove new line characters\n",
    "data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
    "\n",
    "# Remove distracting single quotes\n",
    "data = [re.sub(\"\\'\", \"\", sent) for sent in data]\n",
    "\n",
    "print(data[:1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ed380a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BERTopic code\n",
    "#!pip install bertopic\n",
    "\n",
    "from bertopic import BERTopic\n",
    "model = BERTopic(verbose=True)\n",
    "df1=data[0:500]\n",
    "doc = df1\n",
    "topics, probabilities = model.fit_transform(doc)\n",
    "model.get_topic_freq().head(11)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204f8014",
   "metadata": {},
   "source": [
    "(5) (10 extra points) Compare the results generated by the four topic modeling algorithms, which one is better? You should explain the reasons in details.\n",
    "# Write your answer here (no code needed for this question)\n",
    "\n",
    "From LDA we got coherence score 0.47 and from LSA we got coherence score 0.35 .\n",
    "Bertopic model will took more time to train the data and it is suitable only when we have small set of texts or corpus. Here this dataset contains larger set  of data and it requires more time to process.\n",
    "Hence we conclude that LDA algorithm is better and it has high coherence score 0.47.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
